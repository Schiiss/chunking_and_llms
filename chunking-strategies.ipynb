{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking the Microsoft Build Book of News 2024 for Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries üßë‚Äçüíª\n",
    "\n",
    "We are brining in a few libraries here, most of them are LangChain Libraries:\n",
    "\n",
    "1. Bringing in the CharacterTextSplitter, MarkdownHeaderTextSplitter, and RecursiveCharacterTextSplitter to demonstrate how different chunking strategies impact your retrieval\n",
    "\n",
    "2. AzureAIDocumentIntelligenceLoader to load the PDF and convert to Markdown\n",
    "\n",
    "3. AzureSearch to store our documents after we have chunked them and AzureOpenAIEmbeddings to vectorize the chunks prior to inserting them into Azure Search\n",
    "\n",
    "4. AzureChatOpenAI to interact with GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in Azure OpenAI Embeddings üî¢\n",
    "\n",
    "We are going to leverage OpenAI's embeddings model to vectorize the chunks we generate from the Book Of News document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"embeddings\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking GPT4o A Question Outside of It's Training Dataset ‚ùì\n",
    "\n",
    "GPT4o last received a knowledge update October 2023 so it will not know about Microsoft Build 2024. Let's ask it a question to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "llm.invoke(\"Summarize Azure AI Services announcements in the Microsoft Build Book of News for 2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Chunk Parser Class üßë‚Äçüíª\n",
    "\n",
    "LangChain stores all documents in a Document object and this class will parse that object and display the chunks in an easy to read format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "\n",
    "def parse_documents(data: List[Document]) -> List[Document]:\n",
    "    parsed_documents = []\n",
    "    for doc in data:\n",
    "        parsed_documents.append(Document(page_content=doc.page_content))\n",
    "    return parsed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Book of News PDF Document üîÅ\n",
    "\n",
    "Extract Text/Headers from Book of News PDF Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AzureAIDocumentIntelligenceLoader(file_path=\"C:\\\\Users\\\\conne\\\\development\\\\repos\\\\chunking_for_rag\\\\Book_Of_News.pdf\", api_key=os.environ.get('DOCUMENT_INTELLIGENCE_KEY'), api_endpoint=os.environ.get('DOCUMENT_INTELLIGENCE_ENDPOINT'), api_model=\"prebuilt-layout\")\n",
    "book_of_build = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Extracted Pages from Book of News üëæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_of_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character splitting is the simplest form of chunking and it is the process of dividing your text into N sized chunks and does not take into account the context of the document.\n",
    "\n",
    "Important Concepts:\n",
    "\n",
    "- chunk_size: The number of characters you would like your chunks to be, in our case, 500 characters\n",
    "\n",
    "- chunk_overlap: The amount you would like your chunks to overlap, in our case, 20 characters. This is to ensure context is maintained between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "char_split_chunks = text_splitter.split_documents(book_of_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Chunks üìÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_documents(char_split_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header Splitting & Recursive Character Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are employing a document specific chunking strategy where we analyze the structure of the document and determine the optimal method to chunk. This could be a mix of multiple chunking strategies such as the below method that splits on headers and then splits those chunked headers into 600 character chunks with a 100 character overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "docs_string = book_of_build[0].page_content\n",
    "splits = markdown_splitter.split_text(docs_string)\n",
    "\n",
    "chunk_size = 600\n",
    "chunk_overlap = 100\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "headers_and_rescursive_chunks = text_splitter.split_documents(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Chunks üìÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_documents(headers_and_rescursive_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk on Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last example we are going to chunk on the headers of the document with no further splitting or chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    " \n",
    "docs_string = book_of_build[0].page_content\n",
    "header_chunks = text_splitter.split_text(docs_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Chunks üìÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_documents(header_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Azure Search Indexes üîé\n",
    "\n",
    "This will put the document chunks from all the strategies into a database so we can test our retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "vector_store_password: str = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "\n",
    "index_name: str = \"charsplit\"\n",
    "char_split_vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "index_name: str = \"headerandcharsplit\"\n",
    "header_and_char_split_vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "index_name: str = \"headersplit\"\n",
    "header_split_vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert Chunks to their respective Indexes\n",
    "\n",
    "For each of the chunking strategies we completed above, let's upsert them into Azure Search so we can perform a similarity search on them and see which one performs best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_split_vector_store.add_documents(documents=char_split_chunks)\n",
    "header_and_char_split_vector_store.add_documents(documents=headers_and_rescursive_chunks)\n",
    "header_split_vector_store.add_documents(documents=header_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing üß™\n",
    "\n",
    "Imagine the below retrieved chunks would be fed to an LLM prompt to augment the models training data set. The chunks retireved heavily influence the quality and accuracy of the generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chunking Strategy #1 Character Splitting üß™\n",
    "\n",
    "Here we will execute a search against chunks that were split every 500 characters with a 20 character overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_split_docs = char_split_vector_store.similarity_search(\n",
    "    query=\"Azure AI Services announcements\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(char_split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chunking Strategy #2 Header and Character Splitting üß™\n",
    "\n",
    "The cell below will query chunks that where we split by header and then chunked each header every 600 characters with a 100 character overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_and_recur_split_docs = header_and_char_split_vector_store.similarity_search(\n",
    "    query=\"Azure AI Services announcements\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(header_and_recur_split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chunking Strategy #3 Header Splitting üß™\n",
    "\n",
    "Finally, lets execute a query against chunks where we split on headers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_split_docs = header_split_vector_store.similarity_search(\n",
    "    query=\"Azure AI Services announcements\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(header_split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
