{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking the Microsoft Build Book of News 2024 for Retrieval\n",
    "\n",
    "In this notebook we will demonstrate various chunking strategies and how they influence retrieval.\n",
    "\n",
    "[ChunkViz](https://chunkviz.up.railway.app/) is a great website to see various chunking strategies in a visual way. ‚úÇÔ∏è\n",
    "\n",
    "[2024 BOOK OF NEWS](https://news.microsoft.com/build-2024-book-of-news/) is the document we are going to leverage to demonstrate various chunking strategies.\n",
    "\n",
    "## Questions to consider for chunking\n",
    "\n",
    "1. What is the nature of the content being indexed? Are you working with long documents, such as articles or books, or shorter content, like tweets or instant messages? The answer would dictate both which model would be more suitable for your goal and, consequently, what chunking strategy to apply.\n",
    "\n",
    "2. What are your expectations for the length and complexity of user queries? Will they be short and specific or long and complex? This may inform the way you choose to chunk your content as well so that there‚Äôs a closer correlation between the embedded query and embedded chunks.\n",
    "\n",
    "3. How will the retrieved results be utilized within your specific application? For example, will they be used for semantic search, question answering, summarization, or other purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries üßë‚Äçüíª\n",
    "\n",
    "We are brining in a few libraries here, most of them are LangChain Libraries:\n",
    "\n",
    "1. Bringing in the CharacterTextSplitter, MarkdownHeaderTextSplitter, and RecursiveCharacterTextSplitter to demonstrate how different chunking strategies impact your retrieval\n",
    "\n",
    "2. AzureAIDocumentIntelligenceLoader to load the PDF and convert to Markdown\n",
    "\n",
    "3. AzureOpenAIEmbeddings to vectorize the chunks prior to inserting them into Azure Search and AzureSearch to store our documents after we have chunked and vectorized them\n",
    "\n",
    "4. AzureChatOpenAI to interact with GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking GPT4o A Question Outside of It's Training Dataset ‚ùì\n",
    "\n",
    "GPT4o last received a knowledge update October 2023 so it will not know about Microsoft Build 2024. Let's ask it a question to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As of my last update in October 2023, I don't have access to specific details from the Microsoft Build Book of News for 2024. However, I can provide a general idea of what such announcements might typically include based on past trends and the evolution of Azure AI Services.\\n\\nIn the Microsoft Build Book of News, Azure AI Services announcements often cover:\\n\\n1. **New AI Capabilities and Services**: Introduction of new AI services or significant updates to existing ones, such as enhancements in natural language processing, computer vision, and machine learning models.\\n\\n2. **Integration and Interoperability**: Announcements about improved integration of AI services with other Microsoft products like Azure, Microsoft 365, Dynamics 365, and Power Platform, making it easier for developers to incorporate AI into their applications.\\n\\n3. **Developer Tools and SDKs**: Updates on new or improved tools, SDKs, and APIs that simplify the development and deployment of AI solutions on Azure.\\n\\n4. **Responsible AI and Ethics**: Initiatives and tools aimed at promoting responsible AI usage, including features for bias detection, interpretability, and compliance with ethical guidelines.\\n\\n5. **Customer Success Stories**: Showcasing how various organizations are leveraging Azure AI Services to drive innovation and solve complex problems.\\n\\n6. **Partnerships and Collaborations**: Announcements of new partnerships with other tech companies, research institutions, or industry leaders to advance AI technology and its applications.\\n\\n7. **Performance and Scalability Improvements**: Enhancements in the performance, scalability, and reliability of Azure AI Services, ensuring they can handle larger datasets and more complex models efficiently.\\n\\nFor the most accurate and detailed information, I recommend checking the official Microsoft Build Book of News for 2024 directly on the Microsoft website or their official communication channels.\", response_metadata={'token_usage': {'completion_tokens': 361, 'prompt_tokens': 25, 'total_tokens': 386}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_abc28019ad', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-29c9fcc7-7f8d-4d83-91ca-92a204fe004c-0', usage_metadata={'input_tokens': 25, 'output_tokens': 361, 'total_tokens': 386})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt4o\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "llm.invoke(\"Summarize Azure AI Services announcements in the Microsoft Build Book of News for 2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Class to Display Chunks üßë‚Äçüíª\n",
    "\n",
    "LangChain stores all documents in a Document object and this class will parse that object and display the chunks in an easy to read format.\n",
    "\n",
    "Ex:\n",
    "\n",
    "[Document(page_content='chunk content #1'),\n",
    "Document(page_content='chunk content #2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "\n",
    "def parse_documents(data: List[Document]) -> List[Document]:\n",
    "    parsed_documents = []\n",
    "    for doc in data:\n",
    "        parsed_documents.append(Document(page_content=doc.page_content))\n",
    "    return parsed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Book of News PDF Document üîÅ\n",
    "\n",
    "Extract Text from Book of News PDF Document and convert to markdown. We convert into Markdown so we can take advantage of markdown specific chunking such as splitting on headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AzureAIDocumentIntelligenceLoader(file_path=\"C:\\\\Users\\\\conne\\\\development\\\\repos\\\\chunking_for_rag\\\\Book_Of_News.pdf\", \n",
    "                                           api_key=os.environ.get('DOCUMENT_INTELLIGENCE_KEY'), \n",
    "                                           api_endpoint=os.environ.get('DOCUMENT_INTELLIGENCE_ENDPOINT'))\n",
    "book_of_build = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print output from Document Intelligence üëæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_of_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy #1: Character Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character splitting is the simplest form of chunking and it is the process of dividing your text into N sized chunks and does not take into account the context of the document.\n",
    "\n",
    "- chunk_size: The number of characters you would like your chunks to be, in our case, 500 characters\n",
    "\n",
    "- chunk_overlap: The amount you would like your chunks to overlap, in our case, 20 characters. This is to ensure context is maintained between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "char_split_chunks = text_splitter.split_documents(book_of_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Chunks üìÉ\n",
    "\n",
    "Below are the chunks for the character splitting strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_documents(char_split_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy #2: Header Splitting & Recursive Character Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below method splits on headers and then splits those headers into 600 character chunks with a 100 character overlap leveraging the recursive character splitter. If you recall the recursive text splitter tries to keep all paragraphs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "docs_string = book_of_build[0].page_content\n",
    "splits = markdown_splitter.split_text(docs_string)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600, chunk_overlap=100\n",
    ")\n",
    "headers_and_recursive_chunks = text_splitter.split_documents(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Chunks üìÉ\n",
    "\n",
    "Below is the output from chunking on each header and then recursively splitting within each header "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_documents(headers_and_recursive_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy #3: Chunk on Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last chunking strategy we are going to split on the headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    " \n",
    "docs_string = book_of_build[0].page_content\n",
    "header_chunks = text_splitter.split_text(docs_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Chunks üìÉ\n",
    "\n",
    "Below are the chunked headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_documents(header_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Azure Search Indexes üîé and Azure OpenAI Embeddigs ü§ñ\n",
    "\n",
    "Let's initialize our Azure Search indexes and our embeddings model so we can upsert our chunks to test our retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"embeddings\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "vector_store_password: str = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "\n",
    "index_name: str = \"charsplit\"\n",
    "char_split_vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "index_name: str = \"headerandcharsplit\"\n",
    "header_and_recur_split_vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "index_name: str = \"headersplit\"\n",
    "header_split_vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert Chunks to their respective Indexes\n",
    "\n",
    "For each of the chunking strategies we completed above, let's upsert them into Azure Search so we can perform a similarity search on them and see which one performs best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_split_vector_store.add_documents(documents=char_split_chunks)\n",
    "header_and_recur_split_vector_store.add_documents(documents=headers_and_recursive_chunks)\n",
    "header_split_vector_store.add_documents(documents=header_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing üß™\n",
    "\n",
    "Imagine the below retrieved chunks would be placed into an LLM prompt to augment the models training data set. The chunks retireved heavily influence the quality and accuracy of the generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any Guesses as to which chunking strategy will provide the best results? ‚ùì\n",
    "\n",
    "1. Chunking Strategy #1: Character Splitting\n",
    "\n",
    "2. Chunking Strategy #2: Header and Recursive Character Splitting\n",
    "\n",
    "3. Chunking Strategy #3: Header Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chunking Strategy #1 Character Splitting üß™\n",
    "\n",
    "Here we will execute a search against chunks that were split every 500 characters with a 20 character overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_split_docs = char_split_vector_store.similarity_search(\n",
    "    query=\"Azure AI Services announcements\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(char_split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chunking Strategy #2 Header and Recursive Character Splitting üß™\n",
    "\n",
    "The cell below will query chunks that where we split by header and then chunked each header every 600 characters with a 100 character overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_and_recur_split_docs = header_and_recur_split_vector_store.similarity_search(\n",
    "    query=\"Azure AI Services announcements\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(header_and_recur_split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chunking Strategy #3 Header Splitting üß™\n",
    "\n",
    "Finally, lets execute a query against chunks where we split on headers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_split_docs = header_split_vector_store.similarity_search(\n",
    "    query=\"Azure AI Services announcements\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(header_split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to Consider ü§î\n",
    "\n",
    "1. The retrieved information above would be used to plugin to a prompt to augment the LLM's data set. The quality of the response is heavily influenced by the retrieved data.\n",
    "\n",
    "2. The appropriate chunking allows for efficent retireval of sematically similar chunks and reduces the signal to noise ratio in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
